# ğŸ“š PDF Chat RAG

Uma aplicaÃ§Ã£o de chat com IA baseada em documentos PDF usando RAG (Retrieval-Augmented Generation) com interface estilo ChatGPT.

## ğŸš€ CaracterÃ­sticas

- **Interface ChatGPT**: Interface limpa e profissional similar ao ChatGPT
- **RAG Completo**: Pipeline completo de recuperaÃ§Ã£o e geraÃ§Ã£o baseado em documentos
- **Suporte a MÃºltiplos PDFs**: Processa e indexa mÃºltiplos arquivos simultaneamente
- **ExtraÃ§Ã£o Inteligente**: Usa PyMuPDF + fallback para unstructured (PDFs escaneados)
- **Cache Inteligente**: Evita reprocessamento de arquivos jÃ¡ processados
- **CitaÃ§Ãµes de Fonte**: Mostra exatamente de onde veio cada informaÃ§Ã£o
- **EstatÃ­sticas Detalhadas**: Tempo de resposta, tokens utilizados, etc.
- **Deploy Streamlit Cloud**: Pronto para deploy na nuvem

## ğŸ› ï¸ Tecnologias Utilizadas

- **Frontend**: Streamlit
- **RAG Pipeline**: LangChain
- **Embeddings**: Sentence Transformers (all-MiniLM-L6-v2)
- **Vector Store**: FAISS
- **PDF Processing**: PyMuPDF + Unstructured
- **LLM**: Mistral 7B via Replicate API
- **Cache**: Sistema de cache local com hash MD5

## ğŸ“‹ PrÃ©-requisitos

- Python 3.8+
- Token da API Replicate (opcional, para usar Mistral 7B)

## ğŸš€ InstalaÃ§Ã£o

1. **Clone o repositÃ³rio:**
```bash
git clone <url-do-repositorio>
cd pdf-chat-rag
```

2. **Instale as dependÃªncias:**
```bash
pip install -r requirements.txt
```

3. **Configure as variÃ¡veis de ambiente (opcional):**
```bash
# Crie um arquivo .env
echo "REPLICATE_API_TOKEN=seu_token_aqui" > .env
```

4. **Execute a aplicaÃ§Ã£o:**
```bash
streamlit run app.py
```

## ğŸ“– Como Usar

### 1. Upload de Documentos
- Clique em "Browse files" na barra lateral
- Selecione um ou mais arquivos PDF
- Aguarde o processamento automÃ¡tico

### 2. Fazendo Perguntas
- Digite sua pergunta na caixa de chat
- A IA responderÃ¡ baseada exclusivamente nos documentos
- Veja as fontes utilizadas abaixo de cada resposta

### 3. Controles
- **Recarregar Ãndice**: Carrega Ã­ndice existente
- **Limpar Tudo**: Remove todos os documentos e histÃ³rico

## âš™ï¸ ConfiguraÃ§Ã£o

### Modelo de Embeddings
O modelo padrÃ£o Ã© `sentence-transformers/all-MiniLM-L6-v2`. Para alterar, edite `config.py`:

```python
EMBEDDING_MODEL = "seu_modelo_aqui"
```

### Modelo LLM
O modelo padrÃ£o Ã© Mistral 7B via Replicate. Para usar outro modelo:

1. **Via Replicate**: Altere `LLM_MODEL` em `config.py`
2. **Localmente**: Modifique `rag_pipeline.py` para usar llama.cpp

### ParÃ¢metros RAG
Ajuste os parÃ¢metros em `config.py`:

```python
CHUNK_SIZE = 1000          # Tamanho dos chunks
CHUNK_OVERLAP = 200        # SobreposiÃ§Ã£o entre chunks
MAX_TOKENS_CONTEXT = 1500  # MÃ¡ximo de tokens no contexto
TOP_K_RESULTS = 5          # NÃºmero de chunks recuperados
```

## ğŸ—ï¸ Estrutura do Projeto

```
pdf-chat-rag/
â”œâ”€â”€ app.py                 # AplicaÃ§Ã£o principal Streamlit
â”œâ”€â”€ rag_pipeline.py        # Pipeline RAG completo
â”œâ”€â”€ utils.py              # FunÃ§Ãµes utilitÃ¡rias
â”œâ”€â”€ config.py             # ConfiguraÃ§Ãµes e constantes
â”œâ”€â”€ requirements.txt      # DependÃªncias Python
â”œâ”€â”€ README.md            # DocumentaÃ§Ã£o
â”œâ”€â”€ uploads/             # Arquivos temporÃ¡rios
â”œâ”€â”€ cache/               # Cache de processamento
â””â”€â”€ vector_index/        # Ãndice vetorial FAISS
```

## ğŸ”§ Deploy no Streamlit Cloud

1. **FaÃ§a push para o GitHub**
2. **Conecte no Streamlit Cloud**
3. **Configure variÃ¡veis de ambiente** (se necessÃ¡rio):
   - `REPLICATE_API_TOKEN`: Seu token da Replicate

### ConfiguraÃ§Ã£o do Streamlit Cloud

No painel do Streamlit Cloud, adicione:

```toml
[server]
maxUploadSize = 50
```

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro de Token da API
```
Erro: Token da API Replicate nÃ£o configurado
```
**SoluÃ§Ã£o**: Configure a variÃ¡vel `REPLICATE_API_TOKEN` no Streamlit Cloud ou arquivo `.env`

### PDF nÃ£o processado
```
NÃ£o foi possÃ­vel extrair texto do PDF
```
**SoluÃ§Ã£o**: O PDF pode estar corrompido ou ser uma imagem escaneada. Tente outro arquivo.

### Erro de memÃ³ria
```
Out of memory
```
**SoluÃ§Ã£o**: Reduza `CHUNK_SIZE` ou `TOP_K_RESULTS` em `config.py`

## ğŸ“Š Monitoramento

A aplicaÃ§Ã£o exibe estatÃ­sticas detalhadas:
- **Tempo de resposta**: Tempo para gerar resposta
- **Tokens**: Tokens no contexto e resposta
- **Chunks**: NÃºmero de trechos recuperados
- **Fontes**: Documentos e pÃ¡ginas utilizados

## ğŸ”’ SeguranÃ§a

- Arquivos sÃ£o processados localmente
- Cache Ã© armazenado localmente
- Nenhum dado Ã© enviado para serviÃ§os externos (exceto LLM)
- Arquivos temporÃ¡rios sÃ£o removidos automaticamente

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para detalhes.

## ğŸ™ Agradecimentos

- [LangChain](https://langchain.com/) - Framework RAG
- [Streamlit](https://streamlit.io/) - Interface web
- [FAISS](https://github.com/facebookresearch/faiss) - Vector store
- [Mistral AI](https://mistral.ai/) - Modelo LLM
- [Replicate](https://replicate.com/) - API de inferÃªncia

---

**Desenvolvido com â¤ï¸ para facilitar a consulta de documentos PDF** 